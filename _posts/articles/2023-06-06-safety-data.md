---

layout: post
title: Safety Data: It Takes a Village
author: Sean McGregor and Scott Cambo
date: 2023-06-13 09:00:00 +0900
category: "vulnerabilities"

---

*This is a guest post from [Sean McGregor](https://seanbmcgregor.com/pages/about.html) and [Scott Cambo](https://www.linkedin.com/in/scottallencambo/) submitted to foster discussion and integration of AI safety data across contexts and purposes. The views are their own.*

There has never been more public interest in the dangers posed by AI than we’ve seen in the past year. During that time, AI has become much more ubiquitous and far less constrained. Foundation models and LLMs have inspired the world to attempt to find a use for AI in every possible context. With this ubiquity comes a variety of new ways that AI can cause harm in previously unapproachable scenarios, each motivating a new community to enter the ongoing discussion of what the responsible development and deployment of AI should look like. This **explosion in the number of perspectives is our greatest asset** in centering the development of AI on the needs of humans. However, if we don’t carefully coordinate our efforts to characterize and address AI safety, **we risk ceding leadership of the community interest to only the most sensationalized voices**. In this post, we call on the AI Safety, AI Security, and Responsible AI communities to work with the AI Incident Database to consolidate, harmonize, and merge our best ideas for terminologies and taxonomies so that we can help make our collective voice as responsible stewards of AI as clear and impactful as possible.

The [AI Incident Database](https://incidentdatabase.ai/) (AIID) is a knowledgebase of AI incidents that caused or nearly caused harm. However, the database is more than a collection of news reports on AI gone wrong. It’s designed to be our collective memory of how and why such events unfolded. Each incident that is published on the site has been carefully analyzed by a community of editors to include qualitative information about the context in which the incident occurred. Editors painstakingly comb over every detail they can find to assure the data recorded will allow future users of the AIID to easily query any incident that may be relevant to the AI system they are trying to improve. 

## Disagreement as a Strength

Since AIID depends on a collective effort, there will inevitably be disagreements. Generally, these are good and healthy disagreements about how the data should be structured and what terminology should be adopted. While these disagreements are necessary, they also demand time and effort to resolve through collegial discussion and debate. There are two primary challenges that arise from this: (1) How do we avoid missing an opportunity to record the most impactful details we can about AI incidents? (2) How do we balance the time and effort we put into our respective positions with the time and effort we put into actualizing the ideas we do agree on?

Since its [conception](https://arxiv.org/pdf/2011.08512.pdf), the purpose of the AIID was to provide a union of efforts to catalog and share AI Safety data as they exist without requiring shared definitions and taxonomies. The solution that AIID adopted is derived from the CVE (Common Vulnerabilities and Exposure) system which currently contains over 200,000 publicly-disclosed cybersecurity vulnerabilities spanning a wide variety of industries. While CVE has its own family of taxonomies that it maintains as a high-quality resource across industries, it also allows other systems to use the data underlying this taxonomy to create their own suited to the specific needs of niche domains. Following this example, we are looking to support [cataloging things like deep fakes](https://github.com/responsible-ai-collaborative/aiid/pull/1979) with domain-specific editing processes and metadata.

While the effort to build bespoke AI safety data schemas is motivated by the needs of each unique domain, industry, or community, they often discover and highlight improvements that could be made to the global and standardized taxonomies that AIID maintains. Additionally, sharing these bespoke schemas can help prevent others in the AI development community from needing to duplicate work that has already been done by others.

## How Does this Relate to Generative Red Teaming?

This year the [AI Village will host](https://aivillage.org/generative%20red%20team/generative-red-team/) the largest scale in-person testing of large language models (LLMs) performed to date. People will be asked to press the limits of models and in so doing will learn new things about the circumstances in which models present safety issues. We deliberately use the word "issues," because this is how the AIID scopes the [concept](https://arxiv.org/abs/2211.10384) of the "incident in waiting." Where "incidents" are events that occur in the world, "issues" are intended to inform incident prevention.

Within the "issue" label are the related concepts of "[hazards](https://oecd.ai/en/network-of-experts/working-group/10836)" from the OECD, [risks](https://www.prnewswire.com/news-releases/robust-intelligence-releases-the-ai-risk-database-to-evaluate-supply-chain-risk-in-open-source-models-301784864.html) from Robust Intelligence, and "[vulnerabilities](https://avidml.org/)" from AVID, among others. Various algorithmic assessment organizations also produce these labeled concepts in the course of an audit. All these things vary subtly in definition, application, and use between organizations and we will not attempt to present them. However, **a generative red teaming event will certainly produce all of them**. This is why developing shared data models and collective analysis is of such critical importance. Without developing the capacity to collate and share issues, we will certainly see them realized as incidents.

With appropriate metadata applied to red team outputs, it becomes possible to query across everything from hazards to vulnerabilities and present them in a shared view. In the AIID codebase we plan on scoping each view as queries into systems and context. In the absence of context, issues cannot be bounded, but in the presence of context it becomes possible to inform people of the problems they are likely to produce. For more details, we invite you to discuss the project on the [related GitHub issue](https://github.com/responsible-ai-collaborative/aiid/issues/2047).

## Safety Data Federation

For aforementioned reasons, the AI Incident Database is designed to be *federated.* As we welcome each newcomer to the global discussion of AI safety, we encourage all to use the system that we have been developing collaboratively with input from a multitude of experts in various domains, communities, and contexts where the risk of AI harm has manifested. By using the AIID codebase, any community can define the qualities of an AI incident that seem most important to catalog in service of preventing harm. For example, one group may find it important for AI safety data to include the race or ethnicity of the harmed individual(s) as a requirement for their schema, as is often necessary for analyzing the harms caused by loan approval algorithms or hiring algorithms, while another schema may find it much more important to track the geographic location of the incident. The way we characterize and frame an AI incident by tagging it with the qualitative labels that make the most sense to those who are most familiar with the harm presented are as essential to the utility of the AIID data as the AI incidents themselves. 

AIID was designed from the ground up to accommodate a variety of perspectives on these incidents as well as the review tools and processes needed to consolidate these views whenever possible. While we encourage all to use the work that went into the AIID to help record and share your perspective on AI harms, we also encourage all to take some time to review the contributions that others have made. Perhaps they used different terms derived from the norms of different disciplines that could inspire a new look at the AI incidents most related to your domain. Or perhaps they felt it important to track different aspects of these incidents that would add a new and useful dimension to the risk analysis you are conducting. Or perhaps someone has already done the work that you felt needed to be done and this frees you up to focus on contributing information and ideas to the collective effort that you are uniquely capable of producing. 

AIID has a team of full time web engineers dedicated to making the software needed to collaboratively curate, store, analyze, and share AI safety data from multiple perspectives. We do this work, so that the rest of the community can focus more on AI harms and the best way to respond to them instead of first needing to build a system for cataloging them. We encourage organizations to use our codebase whenever possible and to work with us in finding the best way to pool our collective knowledge and data regarding AI Safety.

## Conclusion

All the big companies are already collecting AI safety datasets and treating them as their secret advantage borne of hardship. Regardless, the Responsible AI Collaborative stands ready to cross-index important data to create that critical safety resource. It is the pipeline of these examples flowing into engineering processes that have the capacity to functionally [govern intelligent systems](https://arxiv.org/abs/2302.07872).
