---
layout: post
title: The Moat for AI
author: Sven Cattell
date: 2025-01-28 09:00:00 +0900
category: "moat"
description: The value of AI companies is largely the security layers they provide.
---

Originally posted [here](https://blog.nbhd.ai/moat). These are my personal opinions. If you want to add to the discussion, we welcome PRs to this website [here](https://github.com/aivillage/aiv_website).



[DeepSeek R1](https://www.deepseek.com/) has arrived and reinforced the fact that no company has a moat on Generative AI. The famous leaked [Google memo](https://www.theverge.com/2023/7/10/23790132/google-memo-moat-ai-leak-demis-hassabis) was accurate and the AI bubble seems to be bursting. However, I think investors have missed the point. The valuable part of the AI industry is the security teams.

AI constantly does things that it’s not supposed to do. A banking chatbot for Chase shouldn’t discuss the weather in Mongolia, nor should it insult its customers. An “agent” that emails company secrets when fed a [doctored image](https://www.darkreading.com/vulnerabilities-threats/llms-open-manipulation-using-doctored-images-audio) is worse than nothing. As social media has found, you do more business when people want to be on your platform. Your AI will do more business when it’s not accidentally [selling trucks for $1](https://gmauthority.com/blog/2023/12/gm-dealer-chat-bot-agrees-to-sell-2024-chevy-tahoe-for-1/). 

To make Generative AI valuable we need guardrails. The moat is the people who manage the AI to keep it on the straight and narrow. This is extremely difficult. The value of AI companies like Anthropic, Google, OpenAI, Meta, and Microsoft are their trust and safety teams. They act as the primary defense securing your model against prompt injections and other threats. You pay for a managed, effective security layer integrated with a top of the line LLM. 

The other option is to do the security layer yourself. Deploying an open source model from a small startup like Mixtral or DeepSeek is opting to do all the security yourself. This might be needed if you require a particular fine tuning. The savings of having a small model efficiently deployed could disappear when you add the costs of making sure it does what it is supposed to and nothing else. Of course what is likely the best option is to pay one of the guardrail companies, like HiddenLayer to do most of it for you.

Either with a large LLM vendor or with a custom built solution, the final security layer will be built and managed by your team in house. Someone at that GM dealership should be monitoring their AI, even if they use the latest and greatest from Anthropic. Generic LLMs can be instructed to stay within bounds, but the end customer defines the bounds. As we know from years of AI security, even if we carefully write the objective function for an AI, a [simple mistake](https://openai.com/index/faulty-reward-functions/) could ruin it.

To bring it to a traditional security context, think of authentication. In this analogy, the service of an AI security team at Google or HiddenLayer is akin to using a trusted third party cryptography library. You can opt to “roll your own crypto”, which is needed in some cases, but only experts should go here. Either way, you will however need security practices internally to manage the passwords, cookies, or tokens yourself. 

While the market may be panicking now, it will come to its senses soon. AI security and the people who manage it is the moat. A company that has agile & secure systems that do what they’re told to do at a reasonable price will beat DeepSeek R1. 
