<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Gradient Attacks - AI Village</title>
<meta name="description" content="Welcome to the second post in the AI Village’s adversarial machine learning series. This one will cover the greedy fast methods that are most commonly used. We will explain what greedy means and why certain choices were made that may not be obvious to newcomers. This is meant as a read-a-long with the papers, we will not be going into a lot of detail but we will focus on the explaining the tricky minutia so that you can understand these papers more easily.">


  <meta name="author" content="Sven Cattell">
  
  <meta property="article:author" content="Sven Cattell">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI Village">
<meta property="og:title" content="Gradient Attacks">
<meta property="og:url" content="http://0.0.0.0:4000/adversarial%20ml/optimization-fgsm/">


  <meta property="og:description" content="Welcome to the second post in the AI Village’s adversarial machine learning series. This one will cover the greedy fast methods that are most commonly used. We will explain what greedy means and why certain choices were made that may not be obvious to newcomers. This is meant as a read-a-long with the papers, we will not be going into a lot of detail but we will focus on the explaining the tricky minutia so that you can understand these papers more easily.">







  <meta property="article:published_time" content="2018-06-12T00:00:00+00:00">





  

  


<link rel="canonical" href="http://0.0.0.0:4000/adversarial%20ml/optimization-fgsm/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "AI Village",
      "url": "http://0.0.0.0:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->




<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--post">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          AI Village
          <span class="site-subtitle">Security of and with AI</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/events/">Our Events</a>
            </li><li class="masthead__menu-item">
              <a href="/blog/">Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/leadership_team/">Leadership Team</a>
            </li><li class="masthead__menu-item">
              <a href="/conduct/">Conduct</a>
            </li><li class="masthead__menu-item">
              <a href="/discord/">Discord Guide</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/default.jpeg" alt="Sven Cattell" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Sven Cattell</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Short bio (change this in _data/authors.yml)</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="firstname.lastname@gmail.edu" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://google.com/" rel="nofollow noopener noreferrer"><i class="fas fa-globe" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <div class="archive">
    
      <h1 id="page-title" class="page__title">Gradient Attacks</h1>
    
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
Posted by Sven Cattell on 12 June 2018
<p>Welcome to the second post in the AI Village’s adversarial machine learning series. This one will cover the greedy fast methods that are most commonly used. We will explain what greedy means and why certain choices were made that may not be obvious to newcomers. This is meant as a read-a-long with the papers, we will not be going into a lot of detail but we will focus on the explaining the tricky minutia so that you can understand these papers more easily.</p>

<p>We have made a library that demonstrates these attacks. There’s a useful template class in there that we’ll be extending <a href="https://github.com/comath/pytorch_adversarial/blob/master/attacks/attackTemplate.py">BaseAttack</a>. It has methods for testing the effectiveness of the attack and visualization to make debugging these attacks easier. However you still need to implement the attacks yourself. All the images and results in this post come from this <a href="https://github.com/comath/pytorch_adversarial/blob/master/gradientAttacksDemo.py">script</a>.</p>

<p>In the rest of that repo you can also find target models and datasets to play around with. We’re using the <a href="https://github.com/comath/pytorch_adversarial/blob/master/attacks/targets/cifar10ResNet.py">ResNet CIFAR10</a> model for these demonstrations, but if you don’t have GPU handy you can use the <a href="https://github.com/comath/pytorch_adversarial/blob/master/attacks/targets/mnistMLP.py">MNIST MLP model</a>. If you’re in a CTF you will be supplied with those models and an associated pickle so that we’re all on the same page.</p>

<h1 id="the-problem-and-optimization">The Problem and Optimization</h1>

<p>Recall the definition:</p>

<p><strong>Definition</strong>: Let \(f: \mathbb{R}^N \to \{L_1,L_2, \dots L_n\}\) be a function whose range is a discrete set of labels, let \(x\) such that \(f(x) = L_i\), and let \(\hat{x}\) such that \(\lVert x-\hat{x} \rVert_p &lt; \epsilon\) such that \(f(\hat{x}) = L_j\) and \(i \neq j\). Then \(\hat{x}\) is an adversarial example with threshold \(\epsilon\).</p>

<p>We want to find the closest adversarial examples, so what we are trying to do is come up with a perturbation \(h = x - \hat{x}\) such that we change the result of the classifier. Since we’re trying to show that these are dumb machines, this perturbation should be the “shortest” or least noticeable. The classic distance is the \(L_2\) norm, but we can also pick two other norms that often come up in machine learning, \(L\_1\) and \(L\_\infty\).</p>

<h2 id="a-quick-review-of-stochastic-gradient-descent">A Quick Review of Stochastic Gradient Descent</h2>

<p>Our classifier usually uses a one hot encoding for the labels of the data. So, an element of our \(i\)th class should be mapped by our classifier to a vector that has a 1 in the \(i\)th slot and a \(0\)s everywhere else. It doesn’t do this perfectly so we use a loss function to gauge the error between the desired one hot output and the true label. What is important here about the loss function is that it maps to the reals, \(\mathbb{R}\). So, with our training data, \((x,y)\) and our neural network we can get the loss of the network at that input point with that label, \(L(N(x),y)\), usually written as \(J(x,y)\). You should think of this as a height map with low points where the model is correct and high points where the model is wrong.</p>

<p>Normally we descend the gradient over the parameters that the network (or any other model that is differentiable) uses, \(\theta\). The variable \(\theta\) is all parameters in the model, the matrix and bias in a fully connected layer, the convolutional filters, lumped together. These are suppressed in the notation \(N(x)\) and \(J(x,y)\) like class attributes, but we can make it functorial and explicit by writing \(N(x,\theta)\) and \(J(x,y,\theta)\). When we train a network we actually travel over the surface defined by \(J(x,y,\theta)\) with respect to \(\theta\).</p>

<p>High error is bad and the direction to increase the error the fastest at \((x\_i,y\_i)\) is the gradient, \(\nabla\_{\theta}J(x\_i,y\_i,\theta)\). Therefore, we go the opposite direction by a step, 
\(\theta\_{i+1}=\theta\_{i}-\epsilon \nabla\_{\theta} J(x\_i,y\_i,\theta\_{i}),\)</p>

<p>where the \(\epsilon\) is a small value we chose. Do this in a randomized loop over all your data several times and you’ve got Stochiastic Gradient Descent. Think of this as rolling down a very complex hill in \(\theta \in \mathbb{R}^D\) where the dimension, \(D\), can be in the millions.</p>

<h2 id="simplest-attack">Simplest Attack</h2>

<p>Since we’re attacking models instead of training them, we consider \(\theta\) a fixed parameter. We want to change \(y\) with respect to \(x\). So what direction does \(\nabla_x J(x,y)\) point in? It points up the hill of error, so to move the output \(N(x)\) fastest away from the correct value in \(x\) space we ascend the gradient.</p>

<p>If we want to move in the direction that will increase the loss the most by a step of \(\epsilon\), we can do this:
\(\hat{x} = x + \epsilon \nabla_x J(x,y)\)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class GradientAttack(BaseAttack):
	def __init__(self, model, loss, epsilon):
		super(GradientAttack, self).__init__()
		self.model = model
		self.loss = loss
		self.epsilon = epsilon

	def forward(x,y_true):
		# Give x a gradient buffer
		x_adv = x.requires_grad_()

		# Build the loss function at J(x,y)
		y = self.model.forward(x_adv)
		J = self.loss(y,y_true)

		# Ensure that the x gradient buffer is 0
		if x_adv.grad is not None:
			x_adv.grad.data.fill_(0)

		# Compute the gradient 
		x_grad = torch.autograd.grad(J, x_adv)[0]

		# Create the adversarial example and ensure 
		x_adv = x_adv + self.epsilon*x_grad

		# Clip the results to ensure we still have a picture
		# This CIFAR dataset ranges from -1 to 1.
		x_adv = torch.clamp(x_adv, -1, 1)

		return x_adv
</code></pre></div></div>

<p>Lets step through this line by line. First we initialize the attack with the parameters we want to use. BaseAttack wants our attack to be implemented in the forward method of torch.nn.Modules, so we do that. We are going to be supplying raw images to this function, so we need to make pytorch allow us to take the gradient of \(J\) with respect to \(x\).</p>

<p>Since this is an <strong>untargeted</strong> attack we primarily care about if the attack changed the label that the model assigns to that sample. In the gradient method’s case we have the following results:</p>

<iframe src="../material/fgsm_images/gradient_method/index.html" frameborder="0" marginwidth="0" marginheight="0" width="100%" height="500" scrolling="no"></iframe>

<p>The three images are, the base classes, the modified classes, and 10 times the difference. In the center we show red for an unsuccessful attack and green for a successful one.</p>

<p>Our attack is quite a failure. Clearly something needs to be fixed. One thing we can do is to iterate our attack until it successfully confounds the classifier, you can try this by adding a loop and making this a simple gradient descent. However this will result in perhaps hundreds of calls to the perhaps computationally intense model, as well as equally many calls to the automatic differentiation package.</p>

<h2 id="normalized-gradient-attack">Normalized Gradient Attack</h2>

<p>One of things to note is that the difference in the gradient method is really faint. This is because at the bottom of the neural network the gradient may be very small. The gradients we see on these samples range in \(L_2\) length from \(0.003\) to \(0.000002\). To put this in context the average distance between two points in the image space (a 3072 dimensional hypercube) is somewhere <a href="http://mathworld.wolfram.com/HypercubeLinePicking.html">between</a> 18 and 40, and not 0.333 like a line. So, we’re actually moving an absolutely tiny amount. To compensate for this, lets normalize our step, i.e. take the same length step every time:
\(\hat{x} = x + \epsilon \frac{\nabla_x J(x,y)}{\lVert \nabla_x J(x,y) \rVert_2}\)</p>

<p>To do this change the line where we make the adversarial example to:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x_norm = x_grad.view(x_grad.size()[0], -1)
x_norm = F.normalize(x_norm)
x_norm = x_norm.view(x_grad.size())

x_adv = x_adv + self.epsilon*x_norm
</code></pre></div></div>

<p>Lets see how well this works:</p>

<iframe src="../material/fgsm_images/normalized_gradient_method/index.html" frameborder="0" marginwidth="0" marginheight="0" width="100%" height="500" scrolling="no"></iframe>

<p>This clearly enjoys a much greater success rate! We move much further. We also move exactly \(\epsilon\) away from the original, so in competitions where the challenge is to minimize the distance between the normal and adversarial points, we can directly tune this parameter. However it is also the only thing we can tune, so there may be a better way.</p>

<h2 id="fast-gradient-sign-method">Fast Gradient Sign Method</h2>

<p>The final modification of the gradient method is inspired by the \(L_\infty\) norm. This is very closely related to image stenography where you can manipulate the last bit of the RGB values per pixel without a human noticing. This is also the usual introduction, it’s more successful than the normalized gradient method, even faster to compute, and easier to code.</p>

<p>Looking back at the normalized gradient method, we can think of it as moving as far as possible in the “best” direction. We’ve taken a greedy guess at the best direction with the gradient, there might be a better direction that’s not the same as the gradient. We then take the biggest step possible, while staying inside the sphere of radius \(\epsilon\). This process happens in a high dimensional sphere, but it’s not too different from the low dimensional one.</p>

<p><img src="../material/fgsm_images/l2constraint.png" height="500" margin-left="auto" margin-right="auto" alt="Spherical Constraint" /></p>

<p>The gradient in this image is the black arrow, the adversarial step is the green one, and the quantity of red is the height of the loss function. We can see that we’ve stepped right to the border. However there’s darker, higher loss areas just outside of the circle.</p>

<p>For \(L_\infty\) we can move further, we are constrained by a high dimensional cube with side lengths \(2\epsilon\) instead of a sphere of radius \(\epsilon\). The corners of the cube are much further out than the surface of the sphere. To see this we can look at the same situation as above, but with a square instead of a circle.</p>

<p><img src="../material/fgsm_images/l_inty_constraint.png" height="500" margin-left="auto" margin-right="auto" alt="Cube Constraint" /></p>

<p>Our best guess is to move to the corners. The space is small and we’re probably on a close to linear slope, so it is a good bet. To do this we don’t normalize the gradient we take the sign of the gradient.
\(\hat{x} = x + \epsilon \text{sign}(\nabla_x J(x,y))\)
This is accomplished by switching out the creation line in the gradient attack with the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x_adv = x + self.epsilon*x_grad.sign_()
</code></pre></div></div>

<p>To see this attack, with the same epsilons as before:</p>

<iframe src="../material/fgsm_images/gradient_sign_method/index.html" frameborder="0" marginwidth="0" marginheight="0" width="100%" height="500" scrolling="no"></iframe>

<p>Clearly the most effective so far. However, you can see that we really should stick to low \(\epsilon\)s else it overwhelms the image.</p>

<h1 id="improvements">Improvements</h1>

<p>We can significantly improve these attacks if we were less greedy and took our time. If we took smaller steps, and constrained our movement we could do get similar success rates but remain bit closer to our original image. The next post will be a little shorter and cover these iterative methods.</p>


<ul class="taxonomy__index">
  
  
    <li>
      <a href="#2018">
        <strong>2018</strong> <span class="taxonomy__count">3</span>
      </a>
    </li>
  
</ul>




  <section id="2018" class="taxonomy__section">
    <h2 class="archive__subtitle">2018</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/adversarial%20ml/optimization-fgsm/" rel="permalink">Gradient Attacks
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Welcome to the second post in the AI Village’s adversarial machine learning series. This one will cover the greedy fast methods that are most commonly used. ...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ethics/max-evil-sjterp/" rel="permalink">Max evil MLsec, why should you care?
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Originally posted on Medium - follow @sarajayneterp and like her article there

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/adversarial%20ml/dimensionality-and-adversarial/" rel="permalink">Dimensionality and Adversarial Examples
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Welcome to AI Village’s series on adversarial examples. This will focus on image classification attacks as they are simpler to work with and this series is m...</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to Top &uarr;</a>
  </section>


  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/aivillage_dc" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
      
        
      
        
      
        
          <li><a href="https://www.twitch.tv/aivillage" rel="nofollow noopener noreferrer"><i class="fab fa-twitch" aria-hidden="true"></i> Twitch</a></li>
        
      
        
          <li><a href="https://www.youtube.com/channel/UCBUw0vVET-kB7LPj_XsNQ9Q" rel="nofollow noopener noreferrer"><i class="fab fa-youtube" aria-hidden="true"></i> YouTube</a></li>
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 AI Village. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
