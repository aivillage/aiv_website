<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://0.0.0.0:4000/aiv/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/aiv/" rel="alternate" type="text/html" /><updated>2021-12-16T20:19:29+00:00</updated><id>http://0.0.0.0:4000/aiv/feed.xml</id><title type="html">AI Village</title><subtitle>Educating people on the use and abuse of AI</subtitle><author><name>Your Name</name></author><entry><title type="html">Gradient Attacks</title><link href="http://0.0.0.0:4000/aiv/adversarial%20ml/optimization-fgsm/" rel="alternate" type="text/html" title="Gradient Attacks" /><published>2018-06-12T00:00:00+00:00</published><updated>2018-06-12T00:00:00+00:00</updated><id>http://0.0.0.0:4000/aiv/adversarial%20ml/optimization-fgsm</id><content type="html" xml:base="http://0.0.0.0:4000/aiv/adversarial%20ml/optimization-fgsm/">&lt;p&gt;Welcome to the second post in the AI Village’s adversarial machine learning series. This one will cover the greedy fast methods that are most commonly used. We will explain what greedy means and why certain choices were made that may not be obvious to newcomers. This is meant as a read-a-long with the papers, we will not be going into a lot of detail but we will focus on the explaining the tricky minutia so that you can understand these papers more easily.&lt;/p&gt;

&lt;p&gt;We have made a library that demonstrates these attacks. There’s a useful template class in there that we’ll be extending &lt;a href=&quot;https://github.com/comath/pytorch_adversarial/blob/master/attacks/attackTemplate.py&quot;&gt;BaseAttack&lt;/a&gt;. It has methods for testing the effectiveness of the attack and visualization to make debugging these attacks easier. However you still need to implement the attacks yourself. All the images and results in this post come from this &lt;a href=&quot;https://github.com/comath/pytorch_adversarial/blob/master/gradientAttacksDemo.py&quot;&gt;script&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the rest of that repo you can also find target models and datasets to play around with. We’re using the &lt;a href=&quot;https://github.com/comath/pytorch_adversarial/blob/master/attacks/targets/cifar10ResNet.py&quot;&gt;ResNet CIFAR10&lt;/a&gt; model for these demonstrations, but if you don’t have GPU handy you can use the &lt;a href=&quot;https://github.com/comath/pytorch_adversarial/blob/master/attacks/targets/mnistMLP.py&quot;&gt;MNIST MLP model&lt;/a&gt;. If you’re in a CTF you will be supplied with those models and an associated pickle so that we’re all on the same page.&lt;/p&gt;

&lt;h1 id=&quot;the-problem-and-optimization&quot;&gt;The Problem and Optimization&lt;/h1&gt;

&lt;p&gt;Recall the definition:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: Let \(f: \mathbb{R}^N \to \{L_1,L_2, \dots L_n\}\) be a function whose range is a discrete set of labels, let \(x\) such that \(f(x) = L_i\), and let \(\hat{x}\) such that \(\lVert x-\hat{x} \rVert_p &amp;lt; \epsilon\) such that \(f(\hat{x}) = L_j\) and \(i \neq j\). Then \(\hat{x}\) is an adversarial example with threshold \(\epsilon\).&lt;/p&gt;

&lt;p&gt;We want to find the closest adversarial examples, so what we are trying to do is come up with a perturbation \(h = x - \hat{x}\) such that we change the result of the classifier. Since we’re trying to show that these are dumb machines, this perturbation should be the “shortest” or least noticeable. The classic distance is the \(L_2\) norm, but we can also pick two other norms that often come up in machine learning, \(L\_1\) and \(L\_\infty\).&lt;/p&gt;

&lt;h2 id=&quot;a-quick-review-of-stochastic-gradient-descent&quot;&gt;A Quick Review of Stochastic Gradient Descent&lt;/h2&gt;

&lt;p&gt;Our classifier usually uses a one hot encoding for the labels of the data. So, an element of our \(i\)th class should be mapped by our classifier to a vector that has a 1 in the \(i\)th slot and a \(0\)s everywhere else. It doesn’t do this perfectly so we use a loss function to gauge the error between the desired one hot output and the true label. What is important here about the loss function is that it maps to the reals, \(\mathbb{R}\). So, with our training data, \((x,y)\) and our neural network we can get the loss of the network at that input point with that label, \(L(N(x),y)\), usually written as \(J(x,y)\). You should think of this as a height map with low points where the model is correct and high points where the model is wrong.&lt;/p&gt;

&lt;p&gt;Normally we descend the gradient over the parameters that the network (or any other model that is differentiable) uses, \(\theta\). The variable \(\theta\) is all parameters in the model, the matrix and bias in a fully connected layer, the convolutional filters, lumped together. These are suppressed in the notation \(N(x)\) and \(J(x,y)\) like class attributes, but we can make it functorial and explicit by writing \(N(x,\theta)\) and \(J(x,y,\theta)\). When we train a network we actually travel over the surface defined by \(J(x,y,\theta)\) with respect to \(\theta\).&lt;/p&gt;

&lt;p&gt;High error is bad and the direction to increase the error the fastest at \((x\_i,y\_i)\) is the gradient, \(\nabla\_{\theta}J(x\_i,y\_i,\theta)\). Therefore, we go the opposite direction by a step, 
\(\theta\_{i+1}=\theta\_{i}-\epsilon \nabla\_{\theta} J(x\_i,y\_i,\theta\_{i}),\)&lt;/p&gt;

&lt;p&gt;where the \(\epsilon\) is a small value we chose. Do this in a randomized loop over all your data several times and you’ve got Stochiastic Gradient Descent. Think of this as rolling down a very complex hill in \(\theta \in \mathbb{R}^D\) where the dimension, \(D\), can be in the millions.&lt;/p&gt;

&lt;h2 id=&quot;simplest-attack&quot;&gt;Simplest Attack&lt;/h2&gt;

&lt;p&gt;Since we’re attacking models instead of training them, we consider \(\theta\) a fixed parameter. We want to change \(y\) with respect to \(x\). So what direction does \(\nabla_x J(x,y)\) point in? It points up the hill of error, so to move the output \(N(x)\) fastest away from the correct value in \(x\) space we ascend the gradient.&lt;/p&gt;

&lt;p&gt;If we want to move in the direction that will increase the loss the most by a step of \(\epsilon\), we can do this:
\(\hat{x} = x + \epsilon \nabla_x J(x,y)\)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class GradientAttack(BaseAttack):
	def __init__(self, model, loss, epsilon):
		super(GradientAttack, self).__init__()
		self.model = model
		self.loss = loss
		self.epsilon = epsilon

	def forward(x,y_true):
		# Give x a gradient buffer
		x_adv = x.requires_grad_()

		# Build the loss function at J(x,y)
		y = self.model.forward(x_adv)
		J = self.loss(y,y_true)

		# Ensure that the x gradient buffer is 0
		if x_adv.grad is not None:
			x_adv.grad.data.fill_(0)

		# Compute the gradient 
		x_grad = torch.autograd.grad(J, x_adv)[0]

		# Create the adversarial example and ensure 
		x_adv = x_adv + self.epsilon*x_grad

		# Clip the results to ensure we still have a picture
		# This CIFAR dataset ranges from -1 to 1.
		x_adv = torch.clamp(x_adv, -1, 1)

		return x_adv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Lets step through this line by line. First we initialize the attack with the parameters we want to use. BaseAttack wants our attack to be implemented in the forward method of torch.nn.Modules, so we do that. We are going to be supplying raw images to this function, so we need to make pytorch allow us to take the gradient of \(J\) with respect to \(x\).&lt;/p&gt;

&lt;p&gt;Since this is an &lt;strong&gt;untargeted&lt;/strong&gt; attack we primarily care about if the attack changed the label that the model assigns to that sample. In the gradient method’s case we have the following results:&lt;/p&gt;

&lt;iframe src=&quot;../material/fgsm_images/gradient_method/index.html&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;100%&quot; height=&quot;500&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The three images are, the base classes, the modified classes, and 10 times the difference. In the center we show red for an unsuccessful attack and green for a successful one.&lt;/p&gt;

&lt;p&gt;Our attack is quite a failure. Clearly something needs to be fixed. One thing we can do is to iterate our attack until it successfully confounds the classifier, you can try this by adding a loop and making this a simple gradient descent. However this will result in perhaps hundreds of calls to the perhaps computationally intense model, as well as equally many calls to the automatic differentiation package.&lt;/p&gt;

&lt;h2 id=&quot;normalized-gradient-attack&quot;&gt;Normalized Gradient Attack&lt;/h2&gt;

&lt;p&gt;One of things to note is that the difference in the gradient method is really faint. This is because at the bottom of the neural network the gradient may be very small. The gradients we see on these samples range in \(L_2\) length from \(0.003\) to \(0.000002\). To put this in context the average distance between two points in the image space (a 3072 dimensional hypercube) is somewhere &lt;a href=&quot;http://mathworld.wolfram.com/HypercubeLinePicking.html&quot;&gt;between&lt;/a&gt; 18 and 40, and not 0.333 like a line. So, we’re actually moving an absolutely tiny amount. To compensate for this, lets normalize our step, i.e. take the same length step every time:
\(\hat{x} = x + \epsilon \frac{\nabla_x J(x,y)}{\lVert \nabla_x J(x,y) \rVert_2}\)&lt;/p&gt;

&lt;p&gt;To do this change the line where we make the adversarial example to:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x_norm = x_grad.view(x_grad.size()[0], -1)
x_norm = F.normalize(x_norm)
x_norm = x_norm.view(x_grad.size())

x_adv = x_adv + self.epsilon*x_norm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Lets see how well this works:&lt;/p&gt;

&lt;iframe src=&quot;../material/fgsm_images/normalized_gradient_method/index.html&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;100%&quot; height=&quot;500&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This clearly enjoys a much greater success rate! We move much further. We also move exactly \(\epsilon\) away from the original, so in competitions where the challenge is to minimize the distance between the normal and adversarial points, we can directly tune this parameter. However it is also the only thing we can tune, so there may be a better way.&lt;/p&gt;

&lt;h2 id=&quot;fast-gradient-sign-method&quot;&gt;Fast Gradient Sign Method&lt;/h2&gt;

&lt;p&gt;The final modification of the gradient method is inspired by the \(L_\infty\) norm. This is very closely related to image stenography where you can manipulate the last bit of the RGB values per pixel without a human noticing. This is also the usual introduction, it’s more successful than the normalized gradient method, even faster to compute, and easier to code.&lt;/p&gt;

&lt;p&gt;Looking back at the normalized gradient method, we can think of it as moving as far as possible in the “best” direction. We’ve taken a greedy guess at the best direction with the gradient, there might be a better direction that’s not the same as the gradient. We then take the biggest step possible, while staying inside the sphere of radius \(\epsilon\). This process happens in a high dimensional sphere, but it’s not too different from the low dimensional one.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../material/fgsm_images/l2constraint.png&quot; height=&quot;500&quot; margin-left=&quot;auto&quot; margin-right=&quot;auto&quot; alt=&quot;Spherical Constraint&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The gradient in this image is the black arrow, the adversarial step is the green one, and the quantity of red is the height of the loss function. We can see that we’ve stepped right to the border. However there’s darker, higher loss areas just outside of the circle.&lt;/p&gt;

&lt;p&gt;For \(L_\infty\) we can move further, we are constrained by a high dimensional cube with side lengths \(2\epsilon\) instead of a sphere of radius \(\epsilon\). The corners of the cube are much further out than the surface of the sphere. To see this we can look at the same situation as above, but with a square instead of a circle.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../material/fgsm_images/l_inty_constraint.png&quot; height=&quot;500&quot; margin-left=&quot;auto&quot; margin-right=&quot;auto&quot; alt=&quot;Cube Constraint&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our best guess is to move to the corners. The space is small and we’re probably on a close to linear slope, so it is a good bet. To do this we don’t normalize the gradient we take the sign of the gradient.
\(\hat{x} = x + \epsilon \text{sign}(\nabla_x J(x,y))\)
This is accomplished by switching out the creation line in the gradient attack with the following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x_adv = x + self.epsilon*x_grad.sign_()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To see this attack, with the same epsilons as before:&lt;/p&gt;

&lt;iframe src=&quot;../material/fgsm_images/gradient_sign_method/index.html&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;100%&quot; height=&quot;500&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Clearly the most effective so far. However, you can see that we really should stick to low \(\epsilon\)s else it overwhelms the image.&lt;/p&gt;

&lt;h1 id=&quot;improvements&quot;&gt;Improvements&lt;/h1&gt;

&lt;p&gt;We can significantly improve these attacks if we were less greedy and took our time. If we took smaller steps, and constrained our movement we could do get similar success rates but remain bit closer to our original image. The next post will be a little shorter and cover these iterative methods.&lt;/p&gt;</content><author><name>Sven Cattell</name></author><category term="adversarial ml" /><summary type="html">Welcome to the second post in the AI Village’s adversarial machine learning series. This one will cover the greedy fast methods that are most commonly used. We will explain what greedy means and why certain choices were made that may not be obvious to newcomers. This is meant as a read-a-long with the papers, we will not be going into a lot of detail but we will focus on the explaining the tricky minutia so that you can understand these papers more easily.</summary></entry><entry><title type="html">Max evil MLsec, why should you care?</title><link href="http://0.0.0.0:4000/aiv/ethics/max-evil-sjterp/" rel="alternate" type="text/html" title="Max evil MLsec, why should you care?" /><published>2018-06-11T00:00:00+00:00</published><updated>2018-06-11T00:00:00+00:00</updated><id>http://0.0.0.0:4000/aiv/ethics/max-evil-sjterp</id><content type="html" xml:base="http://0.0.0.0:4000/aiv/ethics/max-evil-sjterp/">&lt;p&gt;&lt;a href=&quot;https://medium.com/@sarajayneterp/max-evil-mlsec-why-should-you-care-ae3a42bfea52&quot;&gt;Originally posted on Medium&lt;/a&gt; - follow &lt;a href=&quot;https://medium.com/@sarajayneterp&quot;&gt;@sarajayneterp&lt;/a&gt; and like her article there&lt;/p&gt;

&lt;p&gt;MLsec is the intersection of machine learning, artificial intelligence, deep learning and information security. It has an active community (see the MLsec project, Defcon’s AI Village and the CAMLIS conference) and a host of applications including the offensive ones outlined in “The Malicious Use of AI”.&lt;/p&gt;

&lt;p&gt;One of the things we’ve been talking about is what it means to be ethical in this space. MLsec work divides into using AI to attack systems and ecosystems, using AI to defend them, and attacking the AI itself, to change its models or behavior in some way (e.g. the groups that poisoned the Tay chatbot’s inputs were directly attacking its models to make it racist). Ethics applies to each of these.&lt;/p&gt;

&lt;h3 id=&quot;talking-about-ethics&quot;&gt;Talking about Ethics&lt;/h3&gt;

&lt;p&gt;Talking about algorithm ethics is trendy, and outside MLsec, there’s been a lot of recent discussion of ethics and AI. But many of the people talking aren’t practitioners: they don’t build AI systems, or train models or make design decisions on their algorithms. There’s also talk about ethics in infosec because it’s a powerful field that affects many people- and when we twin it with another powerful field (AI), and know how much chaos we could unleash with MLsec, we really need to get its ethics right.&lt;/p&gt;

&lt;p&gt;This discussion needs to come from practitioners: the MLsec equivalents of Cathy O’Neill (who I will love forever for seamlessly weaving analysis of penis sizes with flawed recidivism algorithms and other abuses of people). It still needs to be part of the wider conversations about hype cycles (people not trusting bots, then overtrusting them, then reaching a social compromise with them), data risk, and what happens to people and their societies when they start sharing jobs, space, homes, societal decisions and online society with algorithms and bots (sexbots, slackbots, AI-backed bots etc), but we also need to think about the risks that are unique to our domain.&lt;/p&gt;

&lt;h3 id=&quot;a-simple-definition&quot;&gt;A simple definition&lt;/h3&gt;

&lt;p&gt;There are many philosophy courses on ethics. In my data work, I’ve used a simple definition: ethics is about risk, which has 3 main parts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;how bad the consequences of something are (e.g. death is a risk, but so is having your flight delayed),&lt;/li&gt;
  &lt;li&gt;how likely that thing is to happen (e.g. death in a driverless train is relatively rare) and&lt;/li&gt;
  &lt;li&gt;who it affects (in this case, the system designer, owner, user and other stakeholders, bystanders etc).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Risk also has perceptions: for example, people believe that risks from terrorism are greater than those from train travel, and people’s attitudes to different risks can vary from risk-seeking through risk-neutral to risk-averse.
Ethics is about reducing risk in the sense of reducing the combination of the severity of adverse effects, the likelihood of them happening and the number of people or entities it affects. It’s about not being “that guy” with the technology, and being aware of the potential effects and sideeffects of what we do.&lt;/p&gt;

&lt;h3 id=&quot;ethics-in-mlsec&quot;&gt;Ethics in MLsec&lt;/h3&gt;

&lt;p&gt;One of my hacker heroes is @straithe. Her work on how to hack human behaviors with robots is creative, incredible, and opening up a whole new area of incursion, information exfiltration and potential destruction. She thinks the hard thoughts about mlsec risks, and some of the things she’s talked about recently include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using a kid or dead relative’s voice in phishing phonecalls. Yes, we can do that: anyone who podcasts or posts videos of themselves online is leaving data about their voice, it’s relatively easy to record people talking, and Baidu’s voice-mimicking programs are already good enough to fool someone.&lt;/li&gt;
  &lt;li&gt;Using bots (both online and offline) to build emotional bonds with kids, then ‘killing’ or threatening to ‘kill’ those bots.&lt;/li&gt;
  &lt;li&gt;Using passive-aggressive bots to make people do things against their own self-interests.&lt;/li&gt;
  &lt;li&gt;The bad things here are generally personal (distress etc), but that’s not “max evil” yet. What about these?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Changing people’s access to resources by triggering a series of actions that reduce their social credit scores and adversely change their life (this is possible using small actions and a model of their human network).
Microtargetting groups of people with emotive content, to force or change their behavior (e.g. start riots and larger conflicts: when does political advertising stop and warfare start?).
Taking control of a set of autonomous vehicles (what responsibility do you have if one crashes and kills people?)
Mass unintended consequences from your machine learning system (e.g. unintentional racism in its actions).
Now we’re on a bigger scale, both in numbers and effect. When we talk about chaos, we’re talking about letting loose adaptive algorithms and mobile entities that could potentially do anything from making people cry through to destroying their lives and death. Welcome to my life, where we talk about just how far we could go with a technology on both attack and defense, because often we’re up against adversaries who won’t hesitate to have the evil thoughts and act on them, and someone has to think them in order to counter them. This is normal in infosec, and we need to have the same discussions about things like the limits of red team testing, blue team actions, deception and responsible disclosure.&lt;/p&gt;

&lt;h3 id=&quot;why-we-should-care&quot;&gt;Why we should care&lt;/h3&gt;

&lt;p&gt;People can get hurt in infosec operations. Some of those hurts are small (e.g. the loss of face from being successfully phished); some of them are larger. Some of the damage is to targets, sometimes it’s to your own team, sometimes it’s to people you don’t even know (like the non-trolls who found themselves on the big list of Russian trolls).&lt;/p&gt;

&lt;p&gt;MLsec is infosec on steroids: we have this incredibly powerful, exciting research area. I’ve giggled too at the thought of cute robots getting people to open secure doors for them, and it’s fun to think the evil thoughts, to go to places where most people (or at least people who like to sleep at night) shouldn’t go. But with that power comes responsibility, and our responsibility here is to think about ethics, about societal and moral lines before we unknowingly cross them.&lt;/p&gt;

&lt;h3 id=&quot;some-basic-actions-for-us&quot;&gt;Some basic actions for us:&lt;/h3&gt;

&lt;p&gt;When we build or use models, think about whether they’re “fair” to different demographics. Human-created data is often biased against different groups — we shouldn’t just blindly replicate that.
If our models, bots, robots etc can affect humans, think about what the worst effects could be on them, and whether we’re prepared to accept that risk for them.&lt;/p&gt;

&lt;p&gt;Make our design choices wisely.&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further reading:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Cathy O’Neill — her book “Weapons of Math Destruction” and her mathbabe blog&lt;/li&gt;
  &lt;li&gt;Google’s AI ethics memo&lt;/li&gt;
  &lt;li&gt;Risks and mitigations of releasing data (older work with the Responsible Data Forum)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Sara-Jayne Terp</name></author><category term="ethics" /><summary type="html">Originally posted on Medium - follow @sarajayneterp and like her article there</summary></entry><entry><title type="html">Dimensionality and Adversarial Examples</title><link href="http://0.0.0.0:4000/aiv/adversarial%20ml/dimensionality-and-adversarial/" rel="alternate" type="text/html" title="Dimensionality and Adversarial Examples" /><published>2018-05-01T00:00:00+00:00</published><updated>2018-05-01T00:00:00+00:00</updated><id>http://0.0.0.0:4000/aiv/adversarial%20ml/dimensionality-and-adversarial</id><content type="html" xml:base="http://0.0.0.0:4000/aiv/adversarial%20ml/dimensionality-and-adversarial/">&lt;p&gt;Welcome to AI Village’s series on adversarial examples. This will focus on image classification attacks as they are simpler to work with and this series is meant to explain the attacks to hackers who have an interest in data science. The underlying principles are the same for attacks against malware/spam/security classifiers, though the implementation varies. This post focuses on the core reason why adversarial examples work, the high dimensional space our data occupies.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Adversarial examples are minor modifications made to a piece of data such that when it’s feed into a machine learning model it incorrectly handles the data. This might be a problem in self driving cars. Someone who disliked self driving trucks could release a large adversarial bumper sticker that confused the vision system and leads to self driving trucks crashing. This is also a problem in malware detection, trivial modifications to the raw bytes of a malware file, like changing the header, could lead to a deep learning system misclassifying the malware sample as benign. Providing a proper defense is crucial for a safe deployment of machine learning in our lives. We define it mathematically as:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition:&lt;/strong&gt; Let \(f: \mathbb{R}^N \to \{L_1,L_2, \dots L_n\}\) be a function whose range is a discrete set of labels, let \(x\) such that \(f(x) = L_i\), and let \(\hat{x}\) such that \(% &amp;lt;![CDATA[
\lVert x-\hat{x} \rVert_p &amp;lt; \epsilon %]]&amp;gt;\) such that \(f(\hat{x}) = L_j\) and \(i \neq j\). Then \(\hat{x}\) is an adversarial example with threshold \(\epsilon\).&lt;/p&gt;

&lt;p&gt;This mathematical definition is inadequate to cover everything we’d like to call an adversarial example. It doesn’t cover per-pixel classification or segmentation and most NLP situations. However, much of the literature is focused on this mathematical definition. The definition will look familiar to all former calculus students, it is based on the idea of continuity and it relates adversarial examples to discontinuities. Most machine learning systems are continuous (specifically, almost differentiable) by construction, so them showing discontinuous behavior is alarming. It’s not that the math is wrong, it’s just that these systems are demonstrating something related to &lt;a href=&quot;https://en.wikipedia.org/wiki/Chaos_theory&quot;&gt;chaos&lt;/a&gt;. It’s not the same, but to explain why and why it’s likely intractable, we have to make sense of the curse of dimensionality.&lt;/p&gt;

&lt;h2 id=&quot;the-shape-of-data&quot;&gt;The Shape of Data&lt;/h2&gt;

&lt;p&gt;For MNIST, the images lie in 784 dimensional space, one dimension per pixel. These dimensions have some spatial relation to each other, we like to think of them as a grid. This spatial relationship is important to some machine learning models, like convolutional layers. For us, we will largely ignore the spatial relations when constructing adversarial examples.&lt;/p&gt;

&lt;p&gt;These spatial relations of image vectors can be incorporated into the actual data, an 8 translated around the 28x28 grid remains an 8. This means that “eightness” has at least 2 degrees of freedom, or dimensions. There are more dimensions, small rotations and scaling the 8 up and down a little leave it an 8. These can be incorporated into our training with data-augmentation. But, there are also less obvious ways to modify our chosen 8 and keep it’s “eightness”. For example we can scale the top and bottom holes independently.&lt;/p&gt;

&lt;p&gt;Locally to our chosen eight there may be 7 to 10 directions to manipulate it and keep the inherent “eightness”, without doing something outside of what we’d call obvious. These can be combined, so there’s actually many more than just 10 ways to change it in the direction of “eightness”. It’s just that these are combinations of the 7 to 10 inherent ways. This forms a little k-dimensional disk near our favorite 8. This disk is all ways to manipulate the eight and retain “eightness”. This disk lives in the 784 dimensional space, so thinking of a frisbie doesn’t quite hit the mark. It’s really hard to imagine how small this disk is.&lt;/p&gt;

&lt;p&gt;So, an 8 has a local set of other, valid 8s, near it that looks like a 7 to 10 dimensional disk in $\mathbb{R}^{784}$. These glue together to define a surface of eightness, called a manifold, and mathematicians like to say that “eightness” is a low dimensional manifold embedded in $\mathbb{R}^{784}$. This can knot in on itself, and generally be a mess to work with and visualize (this is what tSNE and spectral embeddings try to solve). This manifold has no volume in $\mathbb{R}^{784}$, just like a line doesn’t have volume in $\mathbb{R}^{2}$ or $\mathbb{R}^{3}$. From a mathematician’s perspective this manifold is a nice smooth set and is identified with “eightness”. We say that the dataset of about 5000 eights is sampled from this manifold, with noise. We hope that our model learns to distinguish between this and other manifolds that represent “oneness”, “twoness” and so on.&lt;/p&gt;

&lt;h3 id=&quot;toroidal-example&quot;&gt;Toroidal Example&lt;/h3&gt;

&lt;iframe src=&quot;http://0.0.0.0:4000/aiv/public/material/dimensionality_pointcloud.html&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;100%&quot; height=&quot;500&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In the above example we’ve sampled our data from a torus with relatively little noise. This is a 2 dimensional surface embedded in 3 dimensions. So, locally we have 2 degrees of freedom to manipulate, giving us a 2 dimensional disk around our chosen point. If you hide the main torus you can see the little area around the disk approximates a plane, it’s pretty flat. What we can do to figure out the disk associated to the point is a local principal component analysis. That is we do a PCA on the nearest few hundred points. This will tell us which directions the local neighborhood varies most in.&lt;/p&gt;

&lt;h2 id=&quot;the-curse-of-dimensionality&quot;&gt;The Curse of Dimensionality&lt;/h2&gt;

&lt;p&gt;The data set is sampled from human drawn eights and is nice and clean. What, traditionally, wasn’t done, was to manipulate the just the top left pixel. This has nothing to do with “eightness” to humans, but the ML system doesn’t “think” like us. To account for this we should throw in even more generated examples where we add random noise to each element of our training set. We know that adversarial examples are close to our image. How many extra generated samples would we need?&lt;/p&gt;

&lt;p&gt;Our images lie in 784 dimensional space, but they are only encoded, per pixel, by 8 bits. Suppose we just wanted to 100% guarantee protection against least significant bit twiddling. We can flip the least significant bit of all 784 dimensions, giving us $\mathbb{2}^{784}$ combinations! There is no way to test even a decent fraction of these. This is the essence of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curse of dimensionality&lt;/a&gt;. Even if we add many thousands of points uniformly sampled from this space to each of our dataset we wouldn’t be able to guarantee protection. To train these models perfectly would be like cracking thousands of completely random 20 character passwords.&lt;/p&gt;

&lt;p&gt;From the literature it seems that the adversarial examples might form their own &lt;a href=&quot;https://openreview.net/pdf?id=B1gJ1L2aW&quot;&gt;subspace&lt;/a&gt;. Say we have an adversarial area close by that is inherently 20 dimensional. This is just like the the local eight-space described above, but comes about from the model, not the data. Because we needed much more data than we could possibly generate there are areas where the machine learning model was not trained to handle. We would like the ML model to generalize to these areas, but if you go off in specific weird directions the models seem to break.&lt;/p&gt;

&lt;p&gt;It’s basically impossible to visualize how these low dimensional spaces look, but think of the 20 adversarial directions as \(2^{20}\) “bad” combinations in the above combinatorial example. So the chance of hitting a bad combination is \(\frac{2^{20}}{2^{784}} = 2^{-764}\), or basically 0. The probability of randomly hitting these is incredibly low, but there are many adversarial generation techniques that could unveil them to an attacker. You don’t know what technique will be used to generate the adversarial sample and you don’t know if they have a new one that you don’t know about. So, the challenge is to train a deep learning system that doesn’t have any adversarial examples close to good data.&lt;/p&gt;

&lt;h3 id=&quot;another-toroidal-example&quot;&gt;Another Toroidal Example&lt;/h3&gt;
&lt;iframe src=&quot;http://0.0.0.0:4000/aiv/public/material/adversarial_pointcloud.html&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;100%&quot; height=&quot;500&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This time if we look at a low dimensional example where we have 2 point clouds for 2 classes, a green class and a blue class. These two point clouds are all that the classifier ever saw while training. We’d hope that the classifier learns two tori, but that’s not what happens. Even in perfect &lt;a href=&quot;https://arxiv.org/abs/1801.02774&quot;&gt;conditions&lt;/a&gt; the classifiers we use just don’t pay attention to the shape of the data very well. The adversarial examples (in red) near the point we’re interested in (indicated by the disk) form a 1 dimensional line, with a bit of noise. This could have been caused by many different artifacts in the interaction between the data and the classifier. Since there’s no data sampled from this area for training, there’s noting to correct the classifier when we input them. This is the low dimensional case, the data in the high dimensional case is far more sparse and there’s many more nooks and cranies for these adversarial examples to live.&lt;/p&gt;

&lt;h2 id=&quot;work-around&quot;&gt;Work Around?&lt;/h2&gt;
&lt;p&gt;We have techniques to generate adversarial examples, so why don’t we generate them and add them to the training set? This is the core idea of adversarial training, first tried &lt;a href=&quot;https://arxiv.org/abs/1412.6572&quot;&gt;here&lt;/a&gt;. Goodfellow, Shlens and Szegedy used a fast means of generating adversarial examples to augment the data set, the Fast Gradient Sign Method (FGSM), to augment the dataset. This ought to fix the model slowly by removing adversarial directions from the model. This, however is not what happens. The model learns to artificially create a steep gradient in a non-adversarial direction (&lt;a href=&quot;https://arxiv.org/abs/1705.07204&quot;&gt;see the ensemble paper&lt;/a&gt;). This leads the FGSM to jump in a safe direction and not an adversarial direction. Thus all this training technique accomplishes is training the neural network to mask it’s gradient. This is easy for it to do as it’s usually heavily over parameterized. The ensemble paper also outlines a defense where you trade off the attacks between many models. This seems to work, but the result may be susceptible to non-traditional adversarial attacks.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To properly defend from adversarial attacks in all forms, not just the strict mathematical definition, is probably going to require some major changes to deep learning (and other forms of machine learning). This is probably the most interesting field to work in as it’s at the heart of why deep learning works. It also has major implications for security applications of deep learning, malware signatures will probably be replaced by deep learning models. The cat and mouse game of bypassing virus scanners will eventually be a question of the security of your model against adversarial examples.&lt;/p&gt;</content><author><name>Sven Cattell</name></author><category term="adversarial ml" /><summary type="html">Welcome to AI Village’s series on adversarial examples. This will focus on image classification attacks as they are simpler to work with and this series is meant to explain the attacks to hackers who have an interest in data science. The underlying principles are the same for attacks against malware/spam/security classifiers, though the implementation varies. This post focuses on the core reason why adversarial examples work, the high dimensional space our data occupies.</summary></entry></feed>